我们通过 libvirt 将 Ceph 块设备镜像与 OpenStack 一起使用，它将 QEMU 接口配置为 librbd。 Ceph 将块设备作为跨服务器的对象进行条带化，这意味着大型 Ceph 块设备通常比独立服务器具有更好的性能！

<center> <img src="https://docs.ceph.com/en/nautilus/_images/51dd72cbc0174196f75f96e252d70d62bfe28ed6392c31d85e5a4975a0f4c564.png"> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图7-4 Ceph/Openstack 技术栈</div> </center>

OpenStack 与 Ceph 的块设备集成的三个部分：

Images：OpenStack Glance 管理 VM 的镜像

Volumes：volume 是块设备。 OpenStack 使用卷来引导 VM，或将卷附加到正在运行的 VM。 OpenStack 使用 Cinder 服务管理卷

VMs：vms 是操作系统磁盘。 默认情况下，当我们启动一个虚拟机时，它的磁盘在管理程序的文件系统上显示为一个文件（通常在 /var/lib/nova/instances/uuid/ 下）

需要注意的是：Ceph 不支持 QCOW2 来托管虚拟机磁盘。 因此，如果我们想在 Ceph 中启动虚拟机（临时后端或从卷启动）， Glance 镜像格式必须为 RAW 格式。

### 创建存储池
	
```bash
ceph osd pool create volumes 128
ceph osd pool create images 128
ceph osd pool create backups 128
ceph osd pool create vms 128
```

不同的存储池应该设置多少PG数可以使用 [PG计算器](https://old.ceph.com/pgcalc/)

### 客户端认证

如果我们开启了`cephx`认证的话，需要给 Openstack 的几个组件设置 client 名称及认证密钥

```bash
ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' mgr 'profile rbd pool=images'
ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' mgr 'profile rbd pool=volumes, profile rbd pool=vms'
ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' mgr 'profile rbd pool=backups'
```

如果我们在增加存储池或者遇到其他需要修改 client 权限的时候，可以使用如下的命令：

```bash
ceph auth caps client.glance mon 'profile rbd' osd 'profile rbd pool=images, profile rbd pool=hdd_pool' mgr 'profile rbd pool=images'
```