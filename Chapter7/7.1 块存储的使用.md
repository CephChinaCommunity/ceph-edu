### 准备工作

我们使用 rbd 命令来创建、查询和删除块设备镜像，或者还可以实现克隆镜像、创建快照、将镜像回滚到快照、查看快照等其他特性。

#### 创建存储池

我们在管理节点上使用`ceph`命令创建一个存储池

```bash
ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-rule-name] [expected-num-objects]
# 示例 创建pg数为4096的存储池，默认3副本
ceph osd pool create hdd_pool 4096 4096
```

#### 创建块设备镜像

在向节点添加块设备之前，必须先在 Ceph 存储集群中创建镜像。

```bash
rbd create --size {megabytes} {pool-name}/{image-name}
# 示例 创建1G的镜像
rbd create --size 1024 hdd_pool/test-image
```

#### 查询块设备镜像

列出指定存储池中的块设备，执行以下操作：

```bash
rbd ls {poolname}
# 示例
rbd ls hdd_pool
```

列出指定池中的延迟删除块设备，执行以下操作：

```bash
rbd trash ls {poolname}
# 示例
rbd trash ls hdd_pool
```

#### 查询镜像信息

```bash
rbd info {pool-name}/{image-name}
# 示例
rbd info hdd_pool/test-image
```

#### 调整块设备的大小

Ceph 块设备映像是精简配置的。 在我们写入数据之前，它们实际上并不使用任何物理存储。 但是它们拥有我们在创建镜像时通过 --size 选项设置的最大容量。 我们可以通过下列操作增加（或减少） Ceph 块设备镜像的最大大小：

```bash
rbd resize --size 2048 hdd_pool/test-image (to increase)
# 注意缩容操作慎用，容易导致数据丢失
rbd resize --size 2048 hdd_pool/test-image --allow-shrink (to decrease)
```

#### 删除块设备

```bash
rbd rm {pool-name}/{image-name}
# 示例
rbd rm hdd_pool/test-image
```

上面的操作是实时的，我们也可以通过`trash`命令来实现块设备的延迟删除。

```bash
# 将镜像移动至回收站
rbd trash mv {pool-name}/{image-name}
# 示例
rbd trash mv hdd_pool/test-image

# 删除回收站内指定镜像
rbd trash rm {pool-name}/{image-id}
# 示例
rbd trash rm hdd_pool/test-image
```

被移动至回收站的镜像具体在多久后删除，可以使用`--expires-at`来设置延迟时间（默认是现在），如果它的延迟时间没有过期，除非使用`--force`参数，否则无法删除。

#### 恢复块设备

当一个块设备镜像被加入延迟删除列表时，实际上是并没有删除的，有点类似于 openstack 的 soft delete。此时，我们可以通过下面的命令恢复这样的块设备。

```bash
rbd trash restore {pool-name}/{image-id}
# 示例
rbd trash restore hdd_pool/test-image
```

甚至我们可以在恢复一个延迟删除的镜像的同时给它修改恢复后的镜像名称

```bash
rbd trash restore hdd_pool/test-image --image new-name
```

### 通过 krbd 使用 

krbd（Kernel RADOS Block Device），它是通过 kernel 模块中的 RBD 模块来实现访问后端存储的，RBD 的驱动程序已经被集成到 Linux 内核（2.6.39 或更高版本），将 Linux 操作系统作为客户端挂载卷的前提是加载 RBD 内核模块。

krbd 访问后端存储的方式一般适用于为物理主机提供的块设备，这种方式是基于内核模块驱动的，可以使用Linux自带的页缓存来提高性能。

#### 块设备挂载

```bash
rbd device map {pool-name}/{image-name}
```

#### 查看已挂载的块设备

```bash
rbd device list
```

#### 解除块设备挂载

```bash
rbd device unmap /dev/rbd/{poolname}/{imagename}
```

### 通过 librbd 使用

librbd是一个访问rbd块存储的库，它是基于librados库进行的更高一层的封装，所以librbd是通过librados库来与块存储的数据进行交互的。

使用librbd访问块存储的方式基本都用于为虚拟机提供块设备的方式，比如在 qemu+kvm 虚拟框架中提供的虚拟机，qemu 可以通过 librbd 来访问后端存储，以下是访问的模型图：

<center> <img src="https://docs.ceph.com/en/nautilus/_images/6e7c4d3d03f5bdd6eddb317a811957957a0f0d55004afadfa9b37bf223887ea1.png"> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图7-2 QEMU 访问模型</div> </center>

由于更多的操作都被 openstack 的命令包含在内，所以我们很少单独使用到 qemu 的命令，但是也有比较常见的操作，就是我们使用 qemu-img 将现有的虚拟机映像转换为 Ceph 块设备映像。 例如，如果我们有一个 qcow2 图像，可以运行：

```bash
qemu-img convert -f qcow2 -O raw centos7.4.qcow2 rbd:hdd_pool/centos7.4
```

这个操作一般可以应用于外部镜像的自定义，比如某个厂商自己自定义的系统，我们希望在虚机上做测试，此时就可以把厂商提供的镜像文件转换成我们虚机可以使用的系统盘。

或者是通过 qumu-img 实现两个 ceph 集群间的块设备迁移，命令如下：

```bash
qemu-img convert -p rbd:{src_pool}/{src_image}:conf={src_conf}:keyring={src_keyring} rbd:{dst_pool}/{dst_image}:conf={dst_conf}:keyring={dst_keyring}
```

这样的跨集群迁移方式，一定要保证迁移双方的块设备都处于没有使用的状态，否则容易导致数据不一致。

### 通过 rbd-nbd 使用

rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障。

NBD相比较于kernel rbd：

-   rbd-ko是根据内核主线走的，升级kernel
-   rbd需要升级到相应的内核，改动太大
-   rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd

通过 nbd 挂载块设备的方式如下：

```bash
rbd-nbd map {pool}/{image}
```

类似 krbd ，挂载完成后会在操作系统内看到新的块设备，只是命名方式与 krbd 挂载的块设备不一样，但是后续的使用方式都是一样的。