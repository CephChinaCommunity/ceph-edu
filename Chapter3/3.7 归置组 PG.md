PG 是一些对象的集合，因为在每个对象的基础上跟踪对象放置位置和对象元数据需要昂贵的代价——即，具有数百万个对象的系统事实上无法跟踪每个对象放置位置。

所以 Ceph 将对象以 PG 为单位进行二次组织。首先，集群的 PG 数目经过人工规划因而严格可控，相反，集群中的对象数目时刻处于动态变化之中，这使得基于 PG 精确控制单个 OSD 乃至整个节点的资源消耗成为可能。此外，因为集群中的 PG 数目远小于对象数目，并且 PG 数目和每个 PG 的 ID 都相对固定，因此以 PG 为单位应用数据备份策略和进行数据同步、迁移等，相较直接以对象为单位而言，难度更小且更加灵活。

### PG 映射

每个存储池都有多个 PG。CRUSH 算法动态地将 PG 映射到 OSD。当 Ceph 客户端存储对象时，CRUSH 会将每个对象映射到一个 PG 上。

将对象映射到 PG 会在 Ceph OSD 守护进程和 Ceph 客户端之间创建一个间接层。 Ceph 存储集群必须能够增长（或收缩）并重新平衡它动态存储对象的位置。 如果 Ceph 客户端知道哪个 Ceph OSD 守护进程拥有哪个对象，就会在 Ceph 客户端和 Ceph OSD 守护进程之间建立连接。 相反，CRUSH 算法将每个对象映射到一个 PG，然后将每个 PG 映射到一个或多个 Ceph OSD 守护进程。 当新的 Ceph OSD 守护进程和底层 OSD 设备上线时，这一间接层允许 Ceph 动态重新平衡。 下图描述了 CRUSH 如何将对象映射到 PG，以及 PG 如何映射到 OSD。

<center> <img src="https://docs.ceph.com/en/nautilus/_images/f592d64bd19e67476c118c14caf9d4e3df61607d25670d5b3e83b45d2f29db99.png"> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图3-5 PG 映射</div> </center>

### PG ID 计算

当 Ceph 客户端访问某个 Ceph Monitor 时，会获取到最新版本的 Cluster Map。通过 Cluster Map，客户端可以进一步获取到集群中所有的 Monitor、OSD 的信息。但是，客户端并不知道文件应该保存到什么位置。

客户端所需的唯一输入是对象名称和存储池名称。 Ceph 将数据存储在存储池中（例如默认的 rbd），当客户端想要存储对象时，它会使用文件名称、哈希数值、存储池的 PG 数量以及存储池的名称来计算 PG ID。Ceph 客户端使用以下步骤来计算 PG ID。

1.  客户端输入需要写入对象的存储池名称和对象名称
    
2.  Ceph 获取对象 ID 并计算其哈希值
    
1.  Ceph 以 PG 的数量为模计算哈希来获取 PG ID
    
4.  Ceph 根据存储池名称获取存储池 ID
    
5.  Ceph 在 PG ID 前加上存储池 ID 组成最终 ID
    
计算对象位置比通过命令行查询对象存储位置要快得多。 CRUSH 算法允许客户端计算对象应该存储在哪里，并使客户端能够联系主 OSD 来存储或检索对象。

### Scrub 数据校验

Ceph 可以清理 PG 中的对象来维护数据一致性，也就是说，Ceph OSD 可以将一个 PG 中的对象元数据与其存储在其他 OSD 中的 PG 中的副本进行比较。 Scrub 操作（通常每天执行）可以捕获 OSD 错误或文件系统错误。OSD 还可以通过逐位比较对象中的数据来执行更深入的清理。 Deep-Scrub（通常每周执行一次）会发现磁盘上在轻度清理中不明显的坏扇区。