当前版本中，ObjectStore 主要有 FileStore 和 BlueStore 两种实现方式。其中，FileStore 由于仍然需要通过操作系统自带的本地文件系统间接管理磁盘，所以所有针对 RADOS 的对象操作，都需要预先转换为能够被本地文件系统识别、符合 POSIX 语义的文件操作，这个转换过程极其繁琐，因而效率低下。

BlueStore 最早在 Jewel 版本中引入，主要用于解决 FileStore 中出现的双写惩罚并提高了性能。自 Kraken 版本开始，BlueStore 开始作为默认的对象存储引擎替代了 FileStore。BlueStore 最初的名称叫 NewStore，是 RocksDB 和标准 POSIX 文件系统的组合。然而，人们很快发现引入 POSIX 文件系统会带来高额开销，这也是社区利用新的对象存储引擎替代 FileStore 的原因之一。

> Block + NewStore = BlewStore = BlueStore

### 3.8.1 FileStore 困局

Filestore 最初被设计为对象存储引擎，使开发人员能够在他们的本地机器上测试 Ceph。 由于它的稳定性，它很快成为标准的对象存储，并在世界各地的生产集群中使用。

最初，FileStore 背后的想法是即将推出的 B 树文件系统 (btrfs)，它提供事务支持，将允许 Ceph 将原子需求卸载到 btrfs。 事务允许应用程序向 btrfs 发送一系列请求，并且只有在所有请求都已提交到稳定存储后才会收到确认。 如果没有事务支持，如果在 Ceph 写操作中途出现中断，数据或元数据可能会丢失或与另一个不同步。

不幸的是，依靠 btrfs 来解决这些问题被证明是一个错误的希望，并且发现了一些限制。 btrfs 仍然可以与文件存储一起使用，但有许多已知问题会影响 Ceph 的稳定性。

最后，事实证明 XFS 是与 FileStore 一起使用的最佳选择，但 XFS 的主要限制是它不支持事务，这意味着 Ceph 无法保证其写入的原子性。对此的解决方案是预写日志。 包括数据和元数据在内的所有写入都将首先写入日志，驻留在原始块设备上。 一旦包含数据和元数据的文件系统确认所有数据已安全刷新到磁盘，日志条目就可以被刷新。这样做的一个好处是，当使用 SSD 来保存 HDD 的日志时，它的作用类似于回写缓存，将写入延迟降低到 SSD 的速度。 但是，如果 FileStore journal 与数据分区驻留在同一存储设备上，那么吞吐量将至少减半。在HDD 做 OSD 的情况下，这会导致性能非常差，因为磁盘磁头不断在磁盘的两个区域之间移动，即使是顺序操作也是如此。尽管基于 SSD 的 OSD 上的 FileStore 不会遭受几乎相同的性能损失，但由于需要写入的数据量翻了一番，它们的吞吐量仍然会减半。在任何一种情况下，这种性能损失都是非常不可取的，而且在闪存的情况下，也会更快地磨损设备，需要更昂贵的写入耐久性闪存。 下图展示了 Filestore 及其日志如何与块设备交互，可以看到所有的数据操作都要经过 Filestore 日志和文件系统日志。

<center> <img src="https://res.weread.qq.com/wrepub/epub_36701159_27"> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图3-6 FileStore 设备交互</div> </center>

Ceph 旨在扩展到 PB 级数据并存储数十亿个对象。 但是，由于存储在单一目录中的文件数量受到合理的的限制，因此引入了进一步的解决方法来帮助限制这种情况。 对象存储在散列目录名称的层次结构中；当其中一个文件夹中的文件数达到设置限制时，该目录将被拆分为更远的级别并移动对象。然而，提高对象枚举的速度需要权衡，当这些目录分裂发生时，它们会影响性能，因为对象被移动到正确的目录中。 在更大的磁盘上，目录数量的增加会给 VFS 缓存带来额外的压力，并可能导致不常访问的对象的额外性能损失。

文件存储中的一个主要性能瓶颈是 XFS 必须开始查找 inodes 和目录，因为它们当前未缓存在 RAM 中。 对于每个 OSD 存储大量对象的场景，目前还没有真正的解决方案，并且随着 Ceph 集群逐渐填满，我们可以观察到集群性能开始下降。

不再在 POSIX 文件系统上存储对象实际上是解决大多数这些问题的唯一方法。

### 3.8.2 BlueStore 破局

BlueStore 旨在解决上述限制。 从 NewStore 的发展来看，很明显，在任何方法中尝试使用 POSIX 文件系统作为底层存储层都会引入一些文件存储中也存在的问题。为了让 Ceph 得到不需要文件系统的开销的存储能力，Ceph 需要对存储设备进行直接的块设备访问。 通过将元数据存储在 RocksDB 中，并将实际对象数据直接存储在块设备上，Ceph 可以更好地控制底层存储，同时还提供更好的性能。

下图展示了Bluestore如何与块设备交互。与 FileStore 不同，数据直接写入块设备，元数据操作由RocksDB处理。

<center> <img src="https://res.weread.qq.com/wrepub/epub_36701159_28"> <br> <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">图3-7 BlueStore 设备交互</div> </center>

与在文件存储中每次写入都完全写入日志并最终写入磁盘不同，在 BlueStore 中，写入的数据部分在大多数情况下直接写入块设备。 这消除了双重写入惩罚，并且在纯旋转磁盘 OSD 上显着提高了性能。然而，如前所述，当旋转磁盘与 SSD 日志结合时，这种双重写入具有减少写入延迟的副作用。 BlueStore 还可以使用基于闪存的存储设备通过延迟写入来降低写入延迟，首先将数据写入 RocksDB WAL，然后将这些条目刷新到磁盘。 与文件存储不同，并非每次写入都写入 WAL，配置参数决定了 I/O 大小截止，以延迟哪些写入。这控制将首先写入 WAL 的 I/O 的大小。 对于旋转磁盘，默认为 32 KB，SSD 默认不延迟写入。 如果写入延迟很重要并且您的 SSD 足够快，那么通过增加此值，您可以增加希望延迟到 WAL 的 I/O 的大小

### 3.8.3 RocksDB

RocksDB 是一种高性能的键值存储，最初是从 LevelDB 分叉出来的，但经过开发，Facebook 继续提供适用于具有低延迟存储设备的多处理器服务器的显着性能改进。 它还具有许多功能增强功能，其中一些已在 BlueStore 中使用。

RocksDB 用于存储有关存储对象的元数据，这些元数据以前由文件存储中的 LevelDB 和 XATTR 组合处理。

BlueStore 利用的 RocksDB 的一个特性是能够将 WAL 存储在更快的存储设备上，这有助于降低 RocksDB 操作的延迟。 这也有望提高 Ceph 的性能，尤其是对于较小的 I/O。 这给出了许多可能的存储布局配置，其中 WAL、DB 和数据可以放置在不同的存储设备上。 这里给出三个例子：

- WAL，DB 和 Data 都在旋转磁盘上
- WAL 和 DB 在 SSD 上，Data 在旋转磁盘上
- WAL 在 Nvme 设备上，DB 在 SSD 上，Data 在旋转磁盘上

### 3.8.4 BlueFS

虽然 BlueStore 的主要驱动力不是使用底层文件系统，但 BlueStore 仍然需要一种方法将 RocksDB 和数据存储在 OSD 磁盘上。 BlueFS 被开发出来，这是一个极其精简的文件系统，它只提供 BlueStore 所需的最小功能集。这也意味着它被设计为以可靠的方式运行 Ceph 提交的精简操作集。 它还消除了使用标准 POSIX 文件系统时可能出现的双日志写入影响的开销。
